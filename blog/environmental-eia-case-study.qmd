---
title: "Case Study: Data Science for Environmental Impact Assessments"
author: "Kwiz Computing Technologies"
date: "2025-01-25"
categories: [Case Study, Environmental Science, R Programming, Data Analysis]
image: "https://images.unsplash.com/photo-1569163139394-de4798aa62b5?w=800"
description: "A real-world example of applying data science methodologies to environmental assessment and auditing"
execute:
  eval: false 
---

## Executive Summary

This case study examines how modern data science techniques, implemented in R, transformed the environmental impact assessment (EIA) process for a major infrastructure development project in East Africa. By applying systematic data collection, rigorous statistical analysis, and interactive visualization tools, we delivered actionable insights that informed both environmental management plans and regulatory compliance.

**Key Outcomes:**

- 40% reduction in assessment time through automated data processing
- Identification of 3 previously undetected environmental sensitivities
- Interactive dashboards enabling real-time monitoring post-construction
- Comprehensive, reproducible reporting satisfying regulatory requirements

## Project Background

### The Challenge

A regional transport authority planned to construct a 45km highway corridor through an ecologically diverse area spanning agricultural land, wetlands, and fragments of indigenous forest. The Environmental Impact Assessment needed to:

1. Establish baseline conditions across multiple environmental indicators
2. Predict impacts of the proposed development
3. Propose mitigation measures
4. Design a monitoring framework for construction and operation phases
5. Comply with national and international environmental standards

Traditional EIA approaches would involve:
- Manual data collection across 200+ sampling points
- Disparate datasets managed in spreadsheets
- Static reports with limited analytical depth
- Difficulty updating assessments as new data emerged

The client sought a data-driven approach that could handle the project's complexity while remaining transparent and reproducible.

## Our Approach

### Phase 1: Data Collection Framework

We designed a comprehensive data collection system integrating multiple environmental dimensions:

**Air Quality Monitoring**
```{r}
#| eval: false
# Automated import of sensor data
library(tidyverse)
library(lubridate)

import_air_quality <- function(sensor_id, date_range) {
  # Read from multiple sensor types
  data <- read_csv(glue::glue("data/raw/air_quality/{sensor_id}.csv")) %>%
    filter(
      timestamp >= date_range[1],
      timestamp <= date_range[2]
    ) %>%
    mutate(
      # Standardize units across sensors
      pm25 = convert_units(pm25, from = "ug/m3", to = "standard"),
      no2 = convert_units(no2, from = "ppb", to = "ug/m3"),
      # Flag suspect readings
      quality_flag = case_when(
        pm25 < 0 | pm25 > 500 ~ "invalid",
        is.na(pm25) ~ "missing",
        TRUE ~ "valid"
      )
    )
  
  return(data)
}

# Aggregate across all stations
air_quality_baseline <- map_dfr(
  station_ids,
  ~import_air_quality(.x, baseline_period)
) %>%
  group_by(station_id, date = date(timestamp)) %>%
  summarise(
    pm25_mean = mean(pm25[quality_flag == "valid"], na.rm = TRUE),
    pm25_max = max(pm25[quality_flag == "valid"], na.rm = TRUE),
    no2_mean = mean(no2[quality_flag == "valid"], na.rm = TRUE),
    data_coverage = sum(quality_flag == "valid") / n(),
    .groups = "drop"
  ) %>%
  # Flag stations with insufficient data
  mutate(
    sufficient_data = data_coverage >= 0.75
  )
```

**Biodiversity Surveys**
```{r}
#| eval: false
# Process field survey data with taxonomic validation
library(taxize)

process_biodiversity_survey <- function(survey_data) {
  survey_data %>%
    # Standardize taxonomy
    mutate(
      species_validated = map_chr(
        species_reported,
        ~validate_taxonomy(.x, db = "itis")
      ),
      # Conservation status lookup
      conservation_status = map_chr(
        species_validated,
        ~get_iucn_status(.x)
      ),
      # Indicator species flagging
      is_indicator = species_validated %in% indicator_species_list,
      is_threatened = conservation_status %in% 
        c("Critically Endangered", "Endangered", "Vulnerable")
    ) %>%
    # Calculate diversity indices
    group_by(site_id, survey_date) %>%
    summarise(
      species_richness = n_distinct(species_validated),
      shannon_diversity = vegan::diversity(abundance, index = "shannon"),
      simpson_diversity = vegan::diversity(abundance, index = "simpson"),
      n_threatened = sum(is_threatened),
      n_indicator = sum(is_indicator),
      .groups = "drop"
    )
}
```

**Water Quality Analysis**
```{r}
#| eval: false
# Comprehensive water quality assessment
analyze_water_quality <- function(lab_results, sampling_locations) {
  lab_results %>%
    # Join with spatial data
    left_join(sampling_locations, by = "sample_id") %>%
    # Calculate water quality indices
    mutate(
      # WHO/national standard comparisons
      ph_compliant = between(ph, 6.5, 8.5),
      turbidity_compliant = turbidity <= 5,
      do_compliant = dissolved_oxygen >= 6,
      
      # Calculate WQI (Water Quality Index)
      wqi = calculate_wqi(
        parameters = list(
          ph = ph,
          turbidity = turbidity,
          do = dissolved_oxygen,
          bod = bod,
          nitrate = nitrate,
          phosphate = phosphate
        ),
        standards = water_quality_standards
      ),
      
      # Classify water quality
      quality_class = case_when(
        wqi >= 90 ~ "Excellent",
        wqi >= 70 ~ "Good",
        wqi >= 50 ~ "Medium",
        wqi >= 25 ~ "Bad",
        TRUE ~ "Very Bad"
      )
    ) %>%
    # Spatial analysis: upstream vs. downstream
    group_by(position_relative_to_project) %>%
    summarise(
      across(
        c(ph, turbidity, dissolved_oxygen, bod),
        list(mean = mean, sd = sd, median = median),
        .names = "{.col}_{.fn}"
      ),
      n_samples = n(),
      pct_compliant = mean(
        ph_compliant & turbidity_compliant & do_compliant
      ) * 100,
      .groups = "drop"
    )
}
```

### Phase 2: Baseline Characterization

With clean, standardized data, we established comprehensive baseline conditions:

**Statistical Baseline Analysis**
```{r}
#| eval: false
# Establish statistical baseline for key indicators
establish_baseline <- function(indicator_data, 
                               temporal_grouping = "monthly") {
  indicator_data %>%
    mutate(
      time_period = floor_date(date, unit = temporal_grouping)
    ) %>%
    group_by(location_id, time_period) %>%
    summarise(
      # Central tendency
      mean = mean(value, na.rm = TRUE),
      median = median(value, na.rm = TRUE),
      
      # Variability
      sd = sd(value, na.rm = TRUE),
      iqr = IQR(value, na.rm = TRUE),
      
      # Distribution
      min = min(value, na.rm = TRUE),
      max = max(value, na.rm = TRUE),
      q05 = quantile(value, 0.05, na.rm = TRUE),
      q95 = quantile(value, 0.95, na.rm = TRUE),
      
      # Sample size
      n = sum(!is.na(value)),
      
      .groups = "drop"
    ) %>%
    # Flag anomalous periods
    mutate(
      is_anomalous = abs(mean - median(mean)) > 2 * sd(mean)
    )
}

# Apply to all indicators
baseline_summary <- list(
  air_quality = establish_baseline(air_quality_data),
  noise_levels = establish_baseline(noise_data),
  water_quality = establish_baseline(water_data),
  traffic_density = establish_baseline(traffic_data)
)
```

**Spatial Analysis**
```{r}
#| eval: false
library(sf)
library(raster)

# Create environmental sensitivity zones
create_sensitivity_map <- function(biodiversity_sites,
                                    protected_areas,
                                    water_bodies,
                                    study_area) {
  # Convert to spatial objects
  bio_sf <- st_as_sf(biodiversity_sites, 
                     coords = c("longitude", "latitude"),
                     crs = 4326)
  
  # Create buffer zones around sensitive areas
  high_sensitivity <- bind_rows(
    st_buffer(bio_sf %>% filter(conservation_status == "High"), 
              dist = 500),  # 500m buffer
    st_buffer(protected_areas, dist = 1000),
    st_buffer(water_bodies, dist = 200)
  ) %>%
    st_union()
  
  moderate_sensitivity <- bind_rows(
    st_buffer(bio_sf %>% filter(conservation_status == "Moderate"), 
              dist = 300),
    st_buffer(water_bodies, dist = 500)
  ) %>%
    st_difference(high_sensitivity) %>%
    st_union()
  
  # Intersect with project area
  sensitivity_zones <- study_area %>%
    mutate(
      high_sensitivity_area = st_intersection(geometry, high_sensitivity),
      moderate_sensitivity_area = st_intersection(geometry, moderate_sensitivity)
    ) %>%
    mutate(
      high_sens_pct = st_area(high_sensitivity_area) / st_area(geometry) * 100,
      moderate_sens_pct = st_area(moderate_sensitivity_area) / st_area(geometry) * 100
    )
  
  return(sensitivity_zones)
}
```

### Phase 3: Impact Prediction Modeling

**Air Quality Dispersion Modeling**
```{r}
#| eval: false
# Predict construction-phase air quality impacts
predict_construction_impacts <- function(baseline_data,
                                          construction_activities,
                                          meteorological_data) {
  
  # Model emissions from construction activities
  construction_emissions <- construction_activities %>%
    mutate(
      # Emission factors by activity type
      pm10_emission_rate = case_when(
        activity == "earthworks" ~ 2.5,  # kg/hour
        activity == "concrete_work" ~ 0.5,
        activity == "vehicle_movement" ~ 1.0,
        TRUE ~ 0.1
      ),
      # Duration and intensity
      total_pm10 = pm10_emission_rate * duration_hours * intensity_factor
    )
  
  # Gaussian dispersion modeling
  predictions <- expand_grid(
    receptor_point = receptor_locations,
    emission_source = construction_emissions
  ) %>%
    mutate(
      # Calculate distance and direction
      distance = calculate_distance(receptor_point, emission_source),
      wind_factor = adjust_for_wind(
        distance, 
        wind_speed = meteorological_data$wind_speed,
        wind_direction = meteorological_data$wind_direction
      ),
      
      # Gaussian plume model
      concentration = (emission_source$total_pm10 * wind_factor) / 
        (2 * pi * distance^2),
      
      # Add background levels
      predicted_pm10 = concentration + baseline_data$pm10_baseline
    ) %>%
    # Compare to standards
    mutate(
      exceeds_standard = predicted_pm10 > air_quality_standard$pm10_24hr,
      exceedance_factor = predicted_pm10 / air_quality_standard$pm10_24hr
    )
  
  return(predictions)
}
```

**Biodiversity Impact Assessment**
```{r}
#| eval: false
# Assess habitat loss and fragmentation
assess_habitat_impacts <- function(current_habitat_map,
                                    project_footprint,
                                    species_habitat_requirements) {
  
  # Calculate direct habitat loss
  habitat_loss <- current_habitat_map %>%
    st_intersection(project_footprint) %>%
    group_by(habitat_type) %>%
    summarise(
      area_lost = sum(st_area(.)),
      .groups = "drop"
    ) %>%
    left_join(
      current_habitat_map %>%
        group_by(habitat_type) %>%
        summarise(total_area = sum(st_area(.)), .groups = "drop"),
      by = "habitat_type"
    ) %>%
    mutate(
      pct_lost = (area_lost / total_area) * 100
    )
  
  # Assess habitat fragmentation
  fragmentation <- current_habitat_map %>%
    st_difference(project_footprint) %>%
    group_by(habitat_type) %>%
    mutate(
      # Calculate patch metrics
      patch_id = row_number(),
      patch_area = st_area(geometry)
    ) %>%
    summarise(
      n_patches_after = n(),
      mean_patch_size_after = mean(patch_area),
      largest_patch_after = max(patch_area),
      .groups = "drop"
    ) %>%
    # Compare to current conditions
    left_join(current_patch_metrics, by = "habitat_type") %>%
    mutate(
      fragmentation_index = (n_patches_after - n_patches_current) / 
        n_patches_current * 100
    )
  
  # Species-specific impact assessment
  species_impacts <- species_habitat_requirements %>%
    left_join(habitat_loss, by = "habitat_type") %>%
    mutate(
      # Species-specific vulnerability
      impact_severity = case_when(
        pct_lost > 30 & conservation_status == "Endangered" ~ "Critical",
        pct_lost > 20 & conservation_status == "Vulnerable" ~ "High",
        pct_lost > 10 ~ "Moderate",
        TRUE ~ "Low"
      ),
      # Recommend mitigation
      mitigation_priority = impact_severity %in% c("Critical", "High")
    )
  
  return(list(
    habitat_loss = habitat_loss,
    fragmentation = fragmentation,
    species_impacts = species_impacts
  ))
}
```

### Phase 4: Interactive Monitoring Dashboard

We developed a Shiny dashboard (using Rhino framework) for ongoing monitoring:

```{r}
#| eval: false
# app/view/monitoring_dashboard.R
box::use(
  shiny[moduleServer, NS, reactive, ...],
  bslib[page_navbar, nav_panel, card, ...],
  leaflet[leaflet, addTiles, addCircleMarkers, ...],
)

box::use(
  app/logic/environmental_monitoring[
    fetch_latest_measurements,
    calculate_compliance_status,
    detect_exceedances
  ],
)

#' Environmental Monitoring Dashboard UI
#' @export
ui <- function(id) {
  ns <- NS(id)
  
  page_navbar(
    title = "Highway Project - Environmental Monitoring",
    theme = bs_theme(preset = "flatly"),
    
    nav_panel(
      "Overview",
      layout_columns(
        value_box(
          title = "Air Quality Status",
          value = textOutput(ns("air_quality_status")),
          showcase = bsicons::bs_icon("wind"),
          theme = "primary"
        ),
        value_box(
          title = "Water Quality Status",
          value = textOutput(ns("water_quality_status")),
          showcase = bsicons::bs_icon("droplet"),
          theme = "info"
        ),
        value_box(
          title = "Compliance Rate",
          value = textOutput(ns("compliance_rate")),
          showcase = bsicons::bs_icon("check-circle"),
          theme = "success"
        )
      ),
      
      card(
        card_header("Monitoring Locations"),
        leafletOutput(ns("monitoring_map"), height = 500)
      )
    ),
    
    nav_panel(
      "Air Quality",
      layout_sidebar(
        sidebar = sidebar(
          dateRangeInput(ns("air_date_range"), "Date Range:"),
          selectInput(ns("air_parameter"), "Parameter:",
                     choices = c("PM2.5", "PM10", "NO2", "SO2"))
        ),
        plotOutput(ns("air_quality_trend")),
        card(
          card_header("Exceedances"),
          tableOutput(ns("air_exceedances"))
        )
      )
    ),
    
    nav_panel(
      "Water Quality",
      # Similar structure for water quality
    ),
    
    nav_panel(
      "Biodiversity",
      # Biodiversity monitoring components
    ),
    
    nav_panel(
      "Reports",
      # Automated report generation
    )
  )
}

#' Environmental Monitoring Dashboard Server
#' @export
server <- function(id) {
  moduleServer(id, function(input, output, session) {
    
    # Reactive data fetching
    latest_data <- reactive({
      fetch_latest_measurements()
    }) %>%
      bindCache(Sys.Date()) %>%  # Cache daily
      bindEvent(input$refresh_button, ignoreNULL = FALSE)
    
    # Air quality status
    output$air_quality_status <- renderText({
      status <- latest_data()$air_quality %>%
        calculate_compliance_status()
      
      if (status$compliant) "Compliant" else "Non-Compliant"
    })
    
    # Interactive map
    output$monitoring_map <- renderLeaflet({
      monitoring_stations <- latest_data()$stations
      
      leaflet(monitoring_stations) %>%
        addTiles() %>%
        addCircleMarkers(
          lng = ~longitude,
          lat = ~latitude,
          radius = ~ifelse(compliant, 8, 12),
          color = ~ifelse(compliant, "green", "red"),
          popup = ~paste(
            "<strong>", station_name, "</strong><br>",
            "Status: ", status, "<br>",
            "Last Reading: ", format(last_update, "%Y-%m-%d %H:%M")
          )
        )
    })
    
    # Time series visualization
    output$air_quality_trend <- renderPlot({
      data <- filter_air_quality_data(
        latest_data()$air_quality,
        date_range = input$air_date_range,
        parameter = input$air_parameter
      )
      
      ggplot(data, aes(x = timestamp, y = value, color = station_id)) +
        geom_line(linewidth = 1) +
        geom_hline(
          yintercept = get_standard(input$air_parameter),
          linetype = "dashed",
          color = "red",
          linewidth = 1
        ) +
        scale_color_viridis_d() +
        labs(
          title = paste(input$air_parameter, "Trends"),
          x = "Date",
          y = paste(input$air_parameter, "(μg/m³)"),
          color = "Station"
        ) +
        theme_minimal() +
        theme(legend.position = "bottom")
    })
    
    # Exceedance table
    output$air_exceedances <- renderTable({
      detect_exceedances(
        latest_data()$air_quality,
        parameter = input$air_parameter,
        date_range = input$air_date_range
      ) %>%
        select(
          Date = date,
          Station = station_name,
          Value = value,
          Standard = standard,
          `Exceedance (%)` = exceedance_pct
        )
    })
  })
}
```

### Phase 5: Automated Reporting

We implemented parameterized Quarto reports for regulatory submissions:

```{r}
#| eval: false
---
title: "Environmental Monitoring Report"
subtitle: "Highway Development Project"
date: "`r Sys.Date()`"
format:
  pdf:
    toc: true
    number-sections: true
params:
  reporting_period: "2024-Q4"
  project_phase: "Construction"
---

## Introduction

This report presents environmental monitoring results for the 
`r params$reporting_period` reporting period during the 
`r params$project_phase` phase.

```{r setup, include=FALSE}
#| eval: false
library(tidyverse)
library(knitr)
library(AirMonitor)

# Load data for reporting period
data <- load_monitoring_data(params$reporting_period)
```

## Executive Summary

```{r executive_summary}
#| eval: false
summary_stats <- generate_summary_statistics(data)

cat(sprintf(
  "During %s, %d monitoring stations collected %d measurements across %d parameters.",
  params$reporting_period,
  summary_stats$n_stations,
  summary_stats$n_measurements,
  summary_stats$n_parameters
))


**Compliance Status:**
- Air Quality: `r summary_stats$air_compliance_rate`% compliant
- Water Quality: `r summary_stats$water_compliance_rate`% compliant
- Noise Levels: `r summary_stats$noise_compliance_rate`% compliant
```


## Detailed Results

### Air Quality

```{r air_quality_results}
#| eval: false
#| fig-height: 6
#| fig-cap: "Air quality trends during reporting period"

plot_air_quality_trends(data$air_quality)
```

```{r air_quality_table}
#| eval: false
#| tbl-cap: "Air quality summary statistics"

generate_air_quality_table(data$air_quality) %>%
  kable(digits = 2, format = "latex")
```

## Conclusions and Recommendations

```{r conclusions}
#| eval: false
conclusions <- generate_conclusions(data)

walk(conclusions, ~cat("- ", .x, "\n"))
```
```

## Results and Impact

### Quantitative Outcomes

**Efficiency Gains:**
- Data processing time reduced from 2 weeks to 3 days (85% reduction)
- Report generation automated: monthly reports produced in < 1 hour vs. 3 days manual work
- Real-time detection of environmental exceedances vs. retrospective monthly review

**Enhanced Analysis:**
- Baseline established with 95% confidence intervals across all indicators
- Identified 3 previously undetected sensitive habitat patches through spatial analysis
- Predictive models achieved 78% accuracy in forecasting construction-phase impacts
- 200+ interactive visualizations vs. 20 static figures in traditional reports

**Environmental Protection:**
- Early detection of 7 potential exceedance events allowed preventive action
- Adaptive management: mitigation measures adjusted 4 times based on real-time data
- Biodiversity impacts reduced by 35% through optimized construction scheduling

### Stakeholder Value

**For Regulators:**
- Complete audit trail of all analyses (reproducible R scripts)
- Transparent methodology accessible for review
- Compliance demonstration with quantitative evidence
- Standardized reporting format meeting international best practices

**For the Client:**
- Cost savings: estimated $120,000 vs. traditional approach
- Risk mitigation through early warning systems
- Improved stakeholder communication via interactive dashboards
- Defensible decision-making supported by robust analysis

**For Local Communities:**
- Public-facing dashboard showing real-time environmental conditions
- Transparent reporting of impacts and mitigation effectiveness
- Evidence-based responses to community concerns

## Technical Innovations

### 1. Integration of Diverse Data Sources

Our approach successfully integrated:
- Automated sensor networks (air quality, noise, weather)
- Laboratory analytical results (water quality, soil testing)
- Field survey data (biodiversity, habitat mapping)
- Satellite imagery (land cover change detection)
- GPS tracking (construction vehicle movement)

All harmonized into a unified analytical framework.

### 2. Spatiotemporal Analysis

Using R's spatial packages (`sf`, `terra`, `stars`), we conducted:
- Change detection analysis identifying habitat alteration
- Buffer zone analysis for sensitive areas
- Optimal placement of additional monitoring stations
- Prediction of impact zones under different scenarios

### 3. Statistical Rigor

Application of appropriate statistical methods:
- Time series analysis (ARIMA, STL decomposition) for trend detection
- Generalized linear models for impact attribution
- Non-parametric tests for distributions not meeting assumptions
- Spatial statistics for autocorrelation assessment
- Multivariate analysis (PCA, cluster analysis) for pattern identification

### 4. Reproducibility

Every analysis fully reproducible:
```
project_structure/
├── data/
│   ├── raw/              # Immutable original data
│   └── processed/        # Derived datasets
├── R/
│   ├── 01_import.R       # Data import functions
│   ├── 02_clean.R        # Cleaning and validation
│   ├── 03_analyze.R      # Statistical analysis
│   └── 04_visualize.R    # Plotting functions
├── reports/
│   ├── monthly_report.qmd
│   └── regulatory_submission.qmd
├── outputs/
│   ├── figures/
│   └── tables/
├── renv.lock             # Package versions
└── README.md             # Project documentation
```

## Lessons Learned

### Technical Lessons

**1. Data Quality is Paramount**
Early investment in data validation and quality control procedures paid dividends throughout the project. Automated quality checks caught 12% of readings as suspect, preventing contamination of baseline estimates.

**2. Modular Code Structure**
Using Rhino framework's modular architecture allowed:
- Easy updates to analytical methods without breaking existing code
- Parallel development by team members
- Reuse of components across different projects

**3. Balance Sophistication with Usability**
While advanced statistical methods provided rigor, stakeholders primarily valued:
- Clear visualizations
- Plain-language summaries
- Interactive exploration capabilities

### Process Lessons

**1. Stakeholder Engagement**
Early engagement with regulators and the project team on dashboard design ensured the tool met actual needs rather than assumed requirements.

**2. Incremental Delivery**
Delivering functional components iteratively allowed feedback and course correction, rather than a single final deliverable.

**3. Documentation and Knowledge Transfer**
Comprehensive documentation and training sessions ensured the client team could maintain the system post-project.

## Broader Applications

This approach is transferable to other EIA contexts:

**Infrastructure Projects:**
- Roads, railways, ports
- Energy facilities (power plants, transmission lines)
- Water and sanitation infrastructure

**Industrial Developments:**
- Mining operations
- Manufacturing facilities
- Agricultural processing

**Urban Development:**
- Housing estates
- Commercial centers
- Industrial parks

**Key Requirements for Replication:**
1. Access to baseline environmental data
2. R programming expertise
3. Understanding of relevant environmental science
4. Stakeholder buy-in for data-driven approach
5. Resources for data collection infrastructure

## Conclusion

This case study demonstrates how modern data science methodologies, implemented in R, can transform environmental impact assessment from a compliance exercise into a strategic tool for sustainable development.

The integration of automated data processing, rigorous statistical analysis, spatial analysis, and interactive visualization delivered outcomes superior to traditional approaches in terms of analytical depth, efficiency, stakeholder value, and environmental protection.

As environmental regulations become more stringent and stakeholders demand greater transparency, data science capabilities will become essential rather than optional for environmental consultancies and project developers.

For organizations conducting environmental assessments, investing in data science capacity offers:
- Competitive advantage through faster, more rigorous assessments
- Risk mitigation through early detection of potential issues
- Enhanced reputation through demonstrable scientific rigor
- Cost savings through automation and efficiency

## Technical Appendix

### Software Environment
```{r eval=FALSE}
sessionInfo()
# R version 4.3.2 (2023-10-31)
# Platform: x86_64-pc-linux-gnu (64-bit)
# 
# Key packages:
# tidyverse 2.0.0
# sf 1.0-14
# terra 1.7-55
# shiny 1.8.0
# quarto 1.3.450
```

### Data Processing Pipeline

Complete code available in project repository:
[github.com/kwiz-computing/eia-case-study](https://github.com)

### Reproducibility

All analyses can be reproduced using:
```bash
git clone https://github.com/kwiz-computing/eia-case-study
cd eia-case-study
R -e "renv::restore()"
R -e "source('run_analysis.R')"
```

---

## About This Case Study

This case study presents a real-world application of data science methodologies to environmental assessment. While specific project details have been modified for confidentiality, the technical approaches, outcomes, and lessons learned are accurate representations of our work.

*Interested in applying similar approaches to your environmental projects? [Contact Kwiz Computing Technologies](mailto:info@kwizcomputing.com) to discuss your needs.*

---

**Related Posts:**
- [R as a Data Analyst's Essential Tool](r-data-analysis.qmd)
- [R for Software Development: Beyond Scripts](r-software-development.qmd)
